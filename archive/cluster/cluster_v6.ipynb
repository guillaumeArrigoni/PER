{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from glob import glob\n",
    "from zennitcrp.crp.attribution import CondAttribution\n",
    "import os\n",
    "import json\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from crp.attribution import CondAttribution\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import torch\n",
    "from torchvision.models.vgg import vgg16_bn\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def analysis_clusters(methods_cluster,\n",
    "                      heatmaps_scaled,\n",
    "                      picture_name,\n",
    "                      cluster_folder_save_path,\n",
    "                      min_cluster : int = 2,\n",
    "                      max_cluster : int = 16):\n",
    "    cluster_range = range(min_cluster, max_cluster)\n",
    "    silhouette_scores = {method: [] for method in methods_cluster}\n",
    "    GMM_dict = {}\n",
    "    KMeans_dict = {}\n",
    "    Spectral_dict = {}\n",
    "    Agglomerative_dict = {}\n",
    "\n",
    "    for n_clusters in cluster_range:\n",
    "        for method_name, clustering_function in methods_cluster.items():\n",
    "            model = clustering_function(n_clusters)\n",
    "            labels = model.fit_predict(heatmaps_scaled)\n",
    "\n",
    "            if len(set(labels)) > 1:\n",
    "                score = silhouette_score(heatmaps_scaled, labels)\n",
    "            else:\n",
    "                score = -1\n",
    "            silhouette_scores[method_name].append(score)\n",
    "\n",
    "            if method_name == GMM_CONST:\n",
    "                GMM_dict[n_clusters] = labels\n",
    "            elif method_name == KMEANS_CONST:\n",
    "                KMeans_dict[n_clusters] = labels\n",
    "            elif method_name == SPECTRAL_CONST:\n",
    "                Spectral_dict[n_clusters] = labels\n",
    "            elif method_name == AGGLOMERATIVE_CONST:\n",
    "                Agglomerative_dict[n_clusters] = labels\n",
    "\n",
    "    general_dict = {}\n",
    "    general_score_dict = {}\n",
    "\n",
    "    for method_name, _ in methods_cluster.items():\n",
    "        if method_name == GMM_CONST:\n",
    "            list_dict_keys = list(GMM_dict.keys())\n",
    "            for key in list_dict_keys:\n",
    "                general_dict[f\"{key}_{GMM_CONST}\"] = np.array(GMM_dict[key])\n",
    "            general_score_dict[f\"{GMM_CONST}\"] = np.array(silhouette_scores[GMM_CONST])\n",
    "        elif method_name == KMEANS_CONST:\n",
    "            list_dict_keys = list(KMeans_dict.keys())\n",
    "            for key in list_dict_keys:\n",
    "                general_dict[f\"{key}_{KMEANS_CONST}\"] = np.array(KMeans_dict[key])\n",
    "            general_score_dict[f\"{KMEANS_CONST}\"] = np.array(silhouette_scores[KMEANS_CONST])\n",
    "        elif method_name == SPECTRAL_CONST:\n",
    "            list_dict_keys = list(Spectral_dict.keys())\n",
    "            for key in list_dict_keys:\n",
    "                general_dict[f\"{key}_{SPECTRAL_CONST}\"] = np.array(Spectral_dict[key])\n",
    "            general_score_dict[f\"{SPECTRAL_CONST}\"] = np.array(silhouette_scores[SPECTRAL_CONST])\n",
    "        elif method_name == AGGLOMERATIVE_CONST:\n",
    "            list_dict_keys = list(Agglomerative_dict.keys())\n",
    "            for key in list_dict_keys:\n",
    "                general_dict[f\"{key}_{AGGLOMERATIVE_CONST}\"] = np.array(Agglomerative_dict[key])\n",
    "            general_score_dict[f\"{AGGLOMERATIVE_CONST}\"] = np.array(silhouette_scores[AGGLOMERATIVE_CONST])\n",
    "    general_score_dict[f\"{MIN_CLUSTER_CONST}\"] = min_cluster\n",
    "    general_score_dict[f\"{MAX_CLUSTER_CONST}\"] = max_cluster\n",
    "\n",
    "    np.savez(f\"{cluster_folder_save_path}/clusters_{picture_name}.npz\", **general_dict)\n",
    "    np.savez(f\"{cluster_folder_save_path}/scores_{picture_name}.npz\", **general_score_dict)\n",
    "    return silhouette_scores, cluster_range"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def plot_silhouette_scores(silhouette_scores, cluster_range, file_name):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for method, scores in silhouette_scores.items():\n",
    "        ax.plot(cluster_range, scores, marker='o', linestyle='-', label=method)\n",
    "\n",
    "    ax.set_xlabel(\"Nombre de Clusters\")\n",
    "    ax.set_ylabel(\"Silhouette Score\")\n",
    "    ax.set_title(f\"Comparaison des Algorithmes de Clustering\\n{file_name}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_and_process_normalized(file_name : str,\n",
    "                                base_path,\n",
    "                                layer_name : str = \"layer_40\"):\n",
    "    data = np.load(f\"{base_path}/heatmap/{file_name}\")\n",
    "    heatmaps = data[layer_name]\n",
    "    heatmaps = np.abs(heatmaps)\n",
    "    heatmaps_scaled = np.zeros_like(heatmaps)\n",
    "    for i in range(heatmaps.shape[0]):\n",
    "        min_val = np.min(heatmaps[i])\n",
    "        max_val = np.max(heatmaps[i])\n",
    "        if max_val > min_val:  # Éviter division par zéro\n",
    "            heatmaps_scaled[i] = (heatmaps[i] - min_val) / (max_val - min_val)\n",
    "    heatmaps_flat = heatmaps_scaled.reshape(heatmaps_scaled.shape[0], -1)\n",
    "    return heatmaps, heatmaps_flat"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, input_tensor, layer_idx, num_features, pred_class):\n",
    "    \"\"\"\n",
    "    Calcule l'importance de chaque feature d'une couche donnée pour une classe prédite.\n",
    "\n",
    "    Arguments :\n",
    "    - model : le modèle VGG16\n",
    "    - input_tensor : l'image d'entrée sous forme de tenseur\n",
    "    - layer_idx : l'index de la couche (ex : 40)\n",
    "    - num_features : le nombre total de features dans cette couche (ex : 512)\n",
    "    - pred_class : la classe prédite initialement\n",
    "\n",
    "    Retourne :\n",
    "    - Un dictionnaire {feature_idx : importance} trié par importance décroissante\n",
    "    \"\"\"\n",
    "    input_tensor.requires_grad = True\n",
    "    # Obtenir la probabilité originale de la classe prédite\n",
    "    with torch.no_grad():\n",
    "        output_original = model(input_tensor)\n",
    "        probs_original = torch.nn.functional.softmax(output_original, dim=1)\n",
    "        original_score = probs_original[0, pred_class].item()\n",
    "\n",
    "    feature_importance = {}\n",
    "\n",
    "    # Désactiver chaque feature une par une et mesurer l'impact\n",
    "    for feature_idx in range(num_features):\n",
    "        def zero_out_feature(module, input, output, feature_idx=feature_idx):\n",
    "            output[:, feature_idx, :, :] = 0  # Désactiver la feature\n",
    "            return output\n",
    "\n",
    "        # Ajouter un hook temporaire\n",
    "        hook = model.features[layer_idx].register_forward_hook(zero_out_feature)\n",
    "\n",
    "        # Faire une prédiction avec la feature désactivée\n",
    "        with torch.no_grad():\n",
    "            output_disabled = model(input_tensor)\n",
    "            probs_disabled = torch.nn.functional.softmax(output_disabled, dim=1)\n",
    "            new_score = probs_disabled[0, pred_class].item()\n",
    "\n",
    "        # Supprimer le hook\n",
    "        hook.remove()\n",
    "\n",
    "        # Calcul de l'importance\n",
    "        importance = original_score - new_score\n",
    "        feature_importance[feature_idx] = float(importance)\n",
    "\n",
    "        # Affichage de progression\n",
    "        #print(f\"Feature {feature_idx+1}/{num_features} - Importance: {importance:.4f}\")\n",
    "\n",
    "    # Trier les features par importance décroissante\n",
    "    sorted_importance = dict(sorted(feature_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_importance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def processPicture(model,\n",
    "                   transform,\n",
    "                   global_dictionary : dict,\n",
    "                   picture_path : str,\n",
    "                   heatmap_folder_save_path : str,\n",
    "                   device : str = \"cpu\"):\n",
    "    local_dictionary = {}\n",
    "    image_name = os.path.splitext(os.path.basename(picture_path))[0]\n",
    "    image = Image.open(picture_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    input_tensor.requires_grad = True\n",
    "\n",
    "    output = model(input_tensor)\n",
    "    pred_class = torch.argmax(output, dim=1).item()\n",
    "    probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    local_dictionary[CLASSE_PREDICTED] = pred_class\n",
    "    local_dictionary[PROBABILITY] = probs[0, pred_class]\n",
    "\n",
    "    composite = EpsilonPlusFlat([SequentialMergeBatchNorm()])\n",
    "    attribution = CondAttribution(model, no_param_grad=True)\n",
    "\n",
    "    features_dict = {}\n",
    "    layers_heatmaps = {}\n",
    "    for layer_idx, num_features in features_per_layer.items():\n",
    "        all_heatmaps = []\n",
    "        num_feature_per_batch = 8\n",
    "        index = 0\n",
    "        borne_sup = 0\n",
    "        while borne_sup != num_features:\n",
    "            borne_sup = min((index+1)*num_feature_per_batch, num_features)\n",
    "            conditions = [{\"y\": [40], \"features.40\": [j]} for j in range(index*num_feature_per_batch, borne_sup)]\n",
    "            heatmaps, _, _, _ = attribution(input_tensor, conditions, composite)\n",
    "            all_heatmaps.append(heatmaps)\n",
    "            index += 1\n",
    "        heatmaps = np.concatenate([heatmap.cpu().numpy() for heatmap in all_heatmaps], axis=0)\n",
    "        #heatmaps = np.concatenate(all_heatmaps, axis=0)\n",
    "        layers_heatmaps[layer_idx] = heatmaps\n",
    "\n",
    "        importance_dict = compute_feature_importance(model, input_tensor, layer_idx=layer_idx, num_features=num_features, pred_class=pred_class)\n",
    "\n",
    "        features_dict[layer_idx] = importance_dict\n",
    "\n",
    "    # Normalisation globale sur toutes les heatmaps\n",
    "    min_value = min([heatmaps.min() for heatmaps in layers_heatmaps.values()])\n",
    "    max_value = max([heatmaps.max() for heatmaps in layers_heatmaps.values()])\n",
    "    max_value = max(abs(min_value), abs(max_value))\n",
    "    min_value = -max_value\n",
    "\n",
    "    save_path = os.path.join(heatmap_folder_save_path, f\"{image_name}.npz\")\n",
    "\n",
    "    save_dict = {f\"layer_{idx_layers}\": np.array(heatmaps) for idx_layers, heatmaps in layers_heatmaps.items()}\n",
    "    np.savez(save_path, **save_dict)\n",
    "\n",
    "    local_dictionary[FEATURES] = features_dict\n",
    "    global_dictionary[image_name] = local_dictionary\n",
    "    return global_dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tensor_to_list(obj):\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.tolist()\n",
    "    raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_filename_without_extension(path: str) -> str:\n",
    "    return os.path.splitext(os.path.basename(path))[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_json_from_data(data, output_filename):\n",
    "    \"\"\"\n",
    "    Fonction qui prend un dictionnaire de données avec des informations sur des images,\n",
    "    et les sauvegarde dans un fichier JSON structuré.\n",
    "\n",
    "    :param data: Dictionnaire avec les données des images.\n",
    "    :param output_filename: Nom du fichier JSON à créer.\n",
    "    \"\"\"\n",
    "    # Créer un dictionnaire pour les données au format désiré\n",
    "    image_data = {}\n",
    "\n",
    "    for image_name, info in data.items():\n",
    "        # Extraire les informations : classe, probabilité, et dictionnaire de features\n",
    "        classe = info[CLASSE_PREDICTED]\n",
    "        probability = info[PROBABILITY]\n",
    "        features = info[FEATURES]\n",
    "\n",
    "        # Ajouter ces informations dans le dictionnaire final\n",
    "        image_data[image_name] = {\n",
    "            CLASSE_PREDICTED: classe,\n",
    "            PROBABILITY: probability,\n",
    "            FEATURES: features\n",
    "        }\n",
    "\n",
    "    # Sauvegarder les données dans un fichier JSON\n",
    "    with open(output_filename, 'w') as json_file:\n",
    "        json.dump(image_data, json_file, indent=4, default=tensor_to_list)\n",
    "    print(f\"Le fichier JSON '{output_filename}' a été créé avec succès.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code necessaire standard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_vgg16 = vgg16_bn(True).to(device)\n",
    "model_vgg16.eval()\n",
    "\n",
    "transform_vgg16 = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Constantes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CLASSE_PREDICTED = \"classe_predicted\"\n",
    "PROBABILITY = \"probability\"\n",
    "FEATURES = \"features\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GMM_CONST = \"GMM\"\n",
    "KMEANS_CONST = \"KMeans\"\n",
    "SPECTRAL_CONST = \"SpectralClustering\"\n",
    "AGGLOMERATIVE_CONST = \"AgglomerativeClustering\"\n",
    "HDBSCAN_CONST = \"HDBSCAN\"\n",
    "MIN_CLUSTER_CONST = \"min_cluster\"\n",
    "MAX_CLUSTER_CONST = \"max_cluster\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO : modifier pour mettre le bon chemin\n",
    "picture_folder_path chemin vers le dossier avec les photos\n",
    "cluster_folder_save_path chemin où sont sauvegarder les .npz des heatmaps et l'importance de chaque features dans un .json\n",
    "heatmap_folder_save_path chemin où sont sauvegarder les .npz des clusters et les scores de chaque méthode de clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_path = \"./data/v6/clusters\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "picture_folder_path = f\"{base_path}/pictures\"\n",
    "assert os.path.exists(picture_folder_path), f\"Le dossier '{picture_folder_path}' n'existe pas.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cluster_folder_save_path = f\"{base_path}/clusters\"\n",
    "os.makedirs(cluster_folder_save_path, exist_ok=True)\n",
    "heatmap_folder_save_path = f\"{base_path}/heatmap\"\n",
    "os.makedirs(heatmap_folder_save_path, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modifier pour rajouter d'autre couches si voulu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_per_layer = {\n",
    "    40: 512\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_dictionary = {}\n",
    "image_paths = glob(os.path.join(picture_folder_path, \"*.jpeg\"))\n",
    "print(f\"Nombre d'images trouvées : {len(image_paths)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modifier pour ne pas traiter certaines images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "avoid_images = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extraction des features pour chaque images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for image_path in image_paths:\n",
    "    global_dictionary = {}\n",
    "    if image_path in avoid_images:\n",
    "        print(f\"Image {image_path} avoid\")\n",
    "        continue\n",
    "    image_name = get_filename_without_extension(image_path)\n",
    "    global_dictionary = processPicture(model=model_vgg16,\n",
    "                                       transform=transform_vgg16,\n",
    "                                       global_dictionary = global_dictionary,\n",
    "                                       picture_path = image_path,\n",
    "                                       heatmap_folder_save_path=heatmap_folder_save_path,\n",
    "                                       device=device)\n",
    "    avoid_images.append(image_path)\n",
    "    create_json_from_data(global_dictionary, f\"{heatmap_folder_save_path}/{image_name}_importance.json\") #importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modifier pour supprimer ou rajouter d'autre méthodes de clustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "methods_cluster = {\n",
    "    GMM_CONST: lambda n: GaussianMixture(n_components=n, random_state=42, covariance_type='diag', reg_covar=1e-3),\n",
    "    KMEANS_CONST: lambda n: KMeans(n_clusters=n, random_state=42),\n",
    "    SPECTRAL_CONST: lambda n: SpectralClustering(n_clusters=n, affinity='nearest_neighbors', random_state=42),\n",
    "    AGGLOMERATIVE_CONST: lambda n: AgglomerativeClustering(n_clusters=n),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_dictionary = {}\n",
    "heatmaps_file_list = glob(os.path.join(heatmap_folder_save_path, \"*.npz\"))\n",
    "print(f\"Nombre de 'npz' trouvées : {len(image_paths)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Traitement de l'analyse de clustering et affichage de l'évaluation des différentes méthodes et nombre de cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for heatmaps_file in heatmaps_file_list:\n",
    "    picture_name = f\"{get_filename_without_extension(heatmaps_file)}\"\n",
    "    file_name_with_extension = os.path.basename(heatmaps_file)\n",
    "    heatmaps, heatmaps_scaled = load_and_process_normalized(file_name_with_extension, base_path)\n",
    "    silhouette_scores, cluster_range = analysis_clusters(methods_cluster = methods_cluster,\n",
    "                                                         heatmaps_scaled = heatmaps_scaled,\n",
    "                                                         picture_name = picture_name,\n",
    "                                                         cluster_folder_save_path = cluster_folder_save_path,\n",
    "                                                         min_cluster=2,\n",
    "                                                         max_cluster=16)\n",
    "    plot_silhouette_scores(silhouette_scores = silhouette_scores,\n",
    "                           cluster_range = cluster_range,\n",
    "                           file_name = picture_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
