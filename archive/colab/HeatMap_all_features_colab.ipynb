{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install matplotlib numpy pillow\n",
    "!pip install zennit crp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from crp.attribution import CondAttribution\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "from torchvision.models.vgg import vgg16_bn\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programmation\\Projet\\ExplicationAI\\venv\\PER\\lib\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "D:\\Programmation\\Projet\\ExplicationAI\\venv\\PER\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = vgg16_bn(True).to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CLASSE_PREDICTED = \"classe_predicted\"\n",
    "PROBABILITY = \"probability\"\n",
    "FEATURES = \"features\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features_per_layer = {\n",
    "    0: 64,\n",
    "    3: 64,\n",
    "    7: 128,\n",
    "    10: 128,\n",
    "    14: 256,\n",
    "    17: 256,\n",
    "    20: 256,\n",
    "    24: 512,\n",
    "    27: 512,\n",
    "    30: 512,\n",
    "    34: 512,\n",
    "    37: 512,\n",
    "    40: 512\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_feature_importance(model, input_tensor, layer_idx, num_features, pred_class):\n",
    "    \"\"\"\n",
    "    Calcule l'importance de chaque feature d'une couche donnée pour une classe prédite.\n",
    "\n",
    "    Arguments :\n",
    "    - model : le modèle VGG16\n",
    "    - input_tensor : l'image d'entrée sous forme de tenseur\n",
    "    - layer_idx : l'index de la couche (ex : 40)\n",
    "    - num_features : le nombre total de features dans cette couche (ex : 512)\n",
    "    - pred_class : la classe prédite initialement\n",
    "\n",
    "    Retourne :\n",
    "    - Un dictionnaire {feature_idx : importance} trié par importance décroissante\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtenir la probabilité originale de la classe prédite\n",
    "    with torch.no_grad():\n",
    "        output_original = model(input_tensor)\n",
    "        probs_original = torch.nn.functional.softmax(output_original, dim=1)\n",
    "        original_score = probs_original[0, pred_class].item()\n",
    "\n",
    "    feature_importance = {}\n",
    "\n",
    "    # Désactiver chaque feature une par une et mesurer l'impact\n",
    "    for feature_idx in range(num_features):\n",
    "        def zero_out_feature(module, input, output, feature_idx=feature_idx):\n",
    "            output[:, feature_idx, :, :] = 0  # Désactiver la feature\n",
    "            return output\n",
    "\n",
    "        # Ajouter un hook temporaire\n",
    "        hook = model.features[layer_idx].register_forward_hook(zero_out_feature)\n",
    "\n",
    "        # Faire une prédiction avec la feature désactivée\n",
    "        with torch.no_grad():\n",
    "            output_disabled = model(input_tensor)\n",
    "            probs_disabled = torch.nn.functional.softmax(output_disabled, dim=1)\n",
    "            new_score = probs_disabled[0, pred_class].item()\n",
    "\n",
    "        # Supprimer le hook\n",
    "        hook.remove()\n",
    "\n",
    "        # Calcul de l'importance\n",
    "        importance = original_score - new_score\n",
    "        feature_importance[feature_idx] = importance\n",
    "\n",
    "        # Affichage de progression\n",
    "        print(f\"Feature {feature_idx+1}/{num_features} - Importance: {importance:.4f}\")\n",
    "\n",
    "    # Trier les features par importance décroissante\n",
    "    sorted_importance = dict(sorted(feature_importance.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    return sorted_importance\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def processPicture(global_dictionary : dict, picture_path : str = \"/content/drive/My Drive/PER/data\", output_pictures_path :str = \"/content/drive/My Drive/PER/save\"):\n",
    "    local_dictionary = {}\n",
    "    image_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    image = Image.open(picture_path).convert(\"RGB\")\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    output = model(input_tensor)\n",
    "    pred_class = torch.argmax(output, dim=1).item()\n",
    "    probs = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    local_dictionary[CLASSE_PREDICTED] = pred_class\n",
    "    local_dictionary[PROBABILITY] = probs[0, pred_class]\n",
    "\n",
    "    composite = EpsilonPlusFlat([SequentialMergeBatchNorm()])\n",
    "    attribution = CondAttribution(model, no_param_grad=True)\n",
    "\n",
    "    features_dict = {}\n",
    "    layers_heatmaps = {}\n",
    "    for layer_idx, num_features in features_per_layer.items():\n",
    "        all_heatmaps = []\n",
    "        num_feature_per_batch = 8\n",
    "        index = 0\n",
    "        borne_sup = 0\n",
    "        while borne_sup != num_features:\n",
    "            borne_sup = min((index+1)*num_feature_per_batch, num_features)\n",
    "            conditions = [{\"y\": [40], \"features.40\": [j]} for j in range(index*num_feature_per_batch, borne_sup)]\n",
    "            heatmaps, _, _, _ = attribution(input_tensor, conditions, composite)\n",
    "            all_heatmaps.append(heatmaps)\n",
    "            index += 1\n",
    "        heatmaps = np.concatenate(all_heatmaps, axis=0)\n",
    "        layers_heatmaps[layer_idx] = heatmaps\n",
    "\n",
    "        importance_dict = compute_feature_importance(model, input_tensor, layer_idx=layer_idx, num_features=num_features, pred_class=pred_class)\n",
    "\n",
    "        features_dict[layer_idx] = importance_dict\n",
    "\n",
    "    # Normalisation globale sur toutes les heatmaps\n",
    "    min_value = min([heatmaps.min() for heatmaps in layers_heatmaps.values()])\n",
    "    max_value = max([heatmaps.max() for heatmaps in layers_heatmaps.values()])\n",
    "    max_value = max(abs(min_value), abs(max_value))\n",
    "    min_value = -max_value\n",
    "\n",
    "    for idx_layers, heatmaps in layers_heatmaps.items():\n",
    "        save_path_folder = os.path.join(output_pictures_path, image_name)\n",
    "        save_path_folder = os.path.join(save_path, str(idx_layers))\n",
    "        os.makedirs(save_path_folder, exist_ok=True)\n",
    "        for idx, heatmap in enumerate(heatmaps):\n",
    "            fig, ax = plt.subplots(figsize=(4, 4))\n",
    "            cax = ax.imshow(heatmap, cmap=\"seismic\", vmin=min_value, vmax=max_value)\n",
    "\n",
    "            cbar = fig.colorbar(cax, ax=ax, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label(\"Activation Score\")\n",
    "            save_path = os.path.join(save_path_folder, f\"{idx_layers}_{idx+1}.jpeg\")\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "    local_dictionary[FEATURES] = features_dict\n",
    "    global_dictionary[image_name] = local_dictionary\n",
    "    return global_dictionary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_json_from_data(data, output_filename):\n",
    "    \"\"\"\n",
    "    Fonction qui prend un dictionnaire de données avec des informations sur des images,\n",
    "    et les sauvegarde dans un fichier JSON structuré.\n",
    "\n",
    "    :param data: Dictionnaire avec les données des images.\n",
    "    :param output_filename: Nom du fichier JSON à créer.\n",
    "    \"\"\"\n",
    "    # Créer un dictionnaire pour les données au format désiré\n",
    "    image_data = {}\n",
    "\n",
    "    for image_name, info in data.items():\n",
    "        # Extraire les informations : classe, probabilité, et dictionnaire de features\n",
    "        classe = info['classe']\n",
    "        probability = info['probability']\n",
    "        features = info['features']\n",
    "\n",
    "        # Ajouter ces informations dans le dictionnaire final\n",
    "        image_data[image_name] = {\n",
    "            'classe': classe,\n",
    "            'probability': probability,\n",
    "            'features': features\n",
    "        }\n",
    "\n",
    "    # Sauvegarder les données dans un fichier JSON\n",
    "    with open(output_filename, 'w') as json_file:\n",
    "        json.dump(image_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Le fichier JSON '{output_filename}' a été créé avec succès.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_folder = \"/content/drive/My Drive/PER/save\"\n",
    "global_dictionary = {}\n",
    "\n",
    "image_paths = glob(os.path.join(input_folder, \"*.jpeg\"))\n",
    "\n",
    "for image_path in image_paths:\n",
    "    global_dictionary = processPicture(global_dictionary, image_path)\n",
    "create_json_from_data(global_dictionary, \"/content/drive/My Drive/PER/data.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
